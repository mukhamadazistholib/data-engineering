{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf9dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\mukhamad azis tholib\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\mukhamad azis tholib\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72505747",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "code() argument 13 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Career\\2023\\Data Engineer Fellowship 2023\\Github\\data-engineering\\spark sql\\code-along\\03_test.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Career/2023/Data%20Engineer%20Fellowship%202023/Github/data-engineering/spark%20sql/code-along/03_test.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\__init__.py:51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JError\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjava_gateway\u001b[39;00m \u001b[39mimport\u001b[39;00m is_instance_of\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m accumulators\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maccumulators\u001b[39;00m \u001b[39mimport\u001b[39;00m Accumulator\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbroadcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msocketserver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mSocketServer\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[0;32m    100\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAccumulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccumulatorParam\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    103\u001b[0m pickleSer \u001b[39m=\u001b[39m PickleSerializer()\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py:71\u001b[0m\n\u001b[0;32m     68\u001b[0m     xrange \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m\n\u001b[0;32m     69\u001b[0m pickle_protocol \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL\n\u001b[1;32m---> 71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m cloudpickle\n\u001b[0;32m     72\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _exception_message, print_exec\n\u001b[0;32m     75\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPickleSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMarshalSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUTF8Deserializer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle.py:209\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m             \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[0;32m    192\u001b[0m                 co\u001b[39m.\u001b[39mco_argcount,\n\u001b[0;32m    193\u001b[0m                 co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m                 (),\n\u001b[0;32m    207\u001b[0m             )\n\u001b[1;32m--> 209\u001b[0m _cell_set_template_code \u001b[39m=\u001b[39m _make_cell_set_template_code()\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcell_set\u001b[39m(cell, value):\n\u001b[0;32m    213\u001b[0m     \u001b[39m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mukhamad Azis Tholib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle.py:172\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(types\u001b[39m.\u001b[39mCodeType, \u001b[39m\"\u001b[39m\u001b[39mco_posonlyargcount\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# pragma: no branch\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mCodeType(\n\u001b[0;32m    173\u001b[0m             co\u001b[39m.\u001b[39;49mco_argcount,\n\u001b[0;32m    174\u001b[0m             co\u001b[39m.\u001b[39;49mco_posonlyargcount,  \u001b[39m# Python3.8 with PEP570\u001b[39;49;00m\n\u001b[0;32m    175\u001b[0m             co\u001b[39m.\u001b[39;49mco_kwonlyargcount,\n\u001b[0;32m    176\u001b[0m             co\u001b[39m.\u001b[39;49mco_nlocals,\n\u001b[0;32m    177\u001b[0m             co\u001b[39m.\u001b[39;49mco_stacksize,\n\u001b[0;32m    178\u001b[0m             co\u001b[39m.\u001b[39;49mco_flags,\n\u001b[0;32m    179\u001b[0m             co\u001b[39m.\u001b[39;49mco_code,\n\u001b[0;32m    180\u001b[0m             co\u001b[39m.\u001b[39;49mco_consts,\n\u001b[0;32m    181\u001b[0m             co\u001b[39m.\u001b[39;49mco_names,\n\u001b[0;32m    182\u001b[0m             co\u001b[39m.\u001b[39;49mco_varnames,\n\u001b[0;32m    183\u001b[0m             co\u001b[39m.\u001b[39;49mco_filename,\n\u001b[0;32m    184\u001b[0m             co\u001b[39m.\u001b[39;49mco_name,\n\u001b[0;32m    185\u001b[0m             co\u001b[39m.\u001b[39;49mco_firstlineno,\n\u001b[0;32m    186\u001b[0m             co\u001b[39m.\u001b[39;49mco_lnotab,\n\u001b[0;32m    187\u001b[0m             co\u001b[39m.\u001b[39;49mco_cellvars,  \u001b[39m# this is the trickery\u001b[39;49;00m\n\u001b[0;32m    188\u001b[0m             (),\n\u001b[0;32m    189\u001b[0m         )\n\u001b[0;32m    190\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[0;32m    192\u001b[0m             co\u001b[39m.\u001b[39mco_argcount,\n\u001b[0;32m    193\u001b[0m             co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m             (),\n\u001b[0;32m    207\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: code() argument 13 must be str, not int"
     ]
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd55afbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Career\\2023\\Data Engineer Fellowship 2023\\Github\\data-engineering\\spark sql\\code-along\\03_test.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Career/2023/Data%20Engineer%20Fellowship%202023/Github/data-engineering/spark%20sql/code-along/03_test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pyspark\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f1cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6d80ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/15 18:01:40 WARN Utils: Your hostname, centaur.local resolves to a loopback address: 127.0.0.1; using 172.20.10.8 instead (on interface en0)\n",
      "23/03/15 18:01:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/15 18:02:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f604529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-15 18:02:13--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.59.248, 52.216.54.184, 52.216.95.93, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.59.248|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.1’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  47.9KB/s    in 0.3s    \n",
      "\n",
      "2023-03-15 18:02:15 (47.9 KB/s) - ‘taxi+_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12342345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n"
     ]
    }
   ],
   "source": [
    "!head taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809464d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36dd996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb547351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fe2bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 328032\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   7.9K Mar 15 17:44 03_test.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff    18K Sep 10  2022 04_pyspark.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff    32K Sep 10  2022 05_taxi_schema.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   8.6K Sep 10  2022 06_spark_sql.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   2.5K Sep 10  2022 06_spark_sql.py\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   7.5K Sep 10  2022 07_groupby_join.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff    11K Sep 10  2022 08_rdds.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   4.3K Sep 10  2022 09_spark_gcs.ipynb\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   2.9K Sep 10  2022 cloud.md\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   559B Sep 10  2022 download_data.sh\n",
      "-rw-r--r--@ 1 rickynauvaldyruliputra  staff   153M Jun 30  2022 fhvhv_tripdata_2020-05.parquet\n",
      "-rw-rw-r--@ 1 rickynauvaldyruliputra  staff   2.6K Sep 10  2022 hw_spark_sql_big_query.py\n",
      "-rw-r--r--@ 1 rickynauvaldyruliputra  staff    12K Aug 17  2016 taxi+_zone_lookup.csv\n",
      "-rw-r--r--@ 1 rickynauvaldyruliputra  staff    12K Aug 17  2016 taxi+_zone_lookup.csv.1\n",
      "drwxr-xr-x@ 6 rickynauvaldyruliputra  staff   192B Mar 15 18:03 \u001b[1m\u001b[36mzones\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f0812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
